{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments implementation\n",
    "Implementing the convolutional neural networks models and experiments implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import keras\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Flatten,Dropout,Add\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "global data_folder,dataset_file\n",
    "data_folder = \"../data/\"\n",
    "dataset_file= [\"cullpdb+profile_6133.npy.gz\",\"cullpdb+profile_5926_filtered.npy.gz\",\"CB513.npy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading procedures\n",
    "#### Explanation\n",
    "Dataset split\n",
    "int the dataset cullpdb+profile_5926.npy.gz\n",
    "the subset  division of the data set is as follows:\n",
    "[0,5430) training\n",
    "[5435,5690) test\n",
    "[5690,5926) validation\n",
    "\n",
    "\n",
    "\n",
    "The subset division of the cullpdb+profile_6133.npy.gz is as follows:\n",
    "The dataset division for the first cullpdb+profile_6133.npy.gz dataset is\n",
    "[0,5600) training\n",
    "[5605,5877) test\n",
    "[5877,6133) validation\n",
    "\n",
    "\n",
    "In both datasets cullpdb+profile_5926.npy.gz and cullpdb+profile_6133.npy.gz\n",
    "The features in the dataset can be found as follows:\n",
    "\n",
    "It is currently in numpy format as a (N protein x k features) matrix.\n",
    "You can reshape it to (N protein x 700 amino acids x 57 features) first.\n",
    "\n",
    "The 57 features are:\n",
    "[0,22): amino acid residues, with the order of 'A', 'C', 'E', 'D', 'G', 'F', 'I', 'H', 'K', 'M', 'L', 'N', 'Q', 'P',\n",
    "                                               'S', 'R', 'T', 'W', 'V', 'Y', 'X','NoSeq'\n",
    "[22,31): Secondary structure labels, with the sequence of 'L', 'B', 'E', 'G', 'I', 'H', 'S', 'T','NoSeq'\n",
    "[31,33): N- and C- terminals;\n",
    "[33,35): relative and absolute solvent accessibility, used only for training.\n",
    "        (absolute accessibility is thresholded at 15; relative accessibility\n",
    "        is normalized by the largest accessibility value in a protein and thresholded at 0.15;\n",
    "        original solvent accessibility is computed by DSSP)\n",
    "[35,57): sequence profile. Note the order of amino acid residues is ACDEFGHIKLMNPQRSTVWXY and\n",
    "         it is different from the order for amino acid residues\n",
    "\n",
    "The last feature of both amino acid residues and secondary structure labels just mark end of the protein sequence.\n",
    "[22,31) and [33,35) are hidden during testing.\n",
    "\n",
    "source: https://www.princeton.edu/~jzthree/datasets/ICML2014/dataset_readme.txt\n",
    "accessed in 29/06/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildScoringMatrix():\n",
    "    \"\"\"\n",
    "    Building protein scoring matrix and creating a dictionary with the aminoacids and each of their corresponding\n",
    "    rows in the matrix\n",
    "    To convert the one hot encoding of the aminoacids for simplicity it can be made a dot product with the matrix\n",
    "    M with the onehot encoding of the aminoacid.\n",
    "    :return: Scoring Matrix M, positions of the aminoacids \n",
    "    \"\"\"\n",
    "    positions = {'A':0,'C':1,'D':2,'E':3,'F':4,'G':5,'H':6,'I':7,'K':8,'L':9,'M':10,'N':11,\n",
    "                 'P':12,'Q':13,'R':14,'S':15,'T':16,'V':17,'W':18,'Y':19}\n",
    "    M=[[4,   0, -1, -2,  0, -1, -2, -3, -1, -1,  1,  0, -2, -2, -1, -2, -1, -2, -2, -1],\n",
    "       [-1, -2, -3 ,-3 ,-3 ,-3, -4, -4,  8, -3, -1, -1, -3, -2, -1, -2, -1, -2, -2, -1],\n",
    "       [3,   2, -2 ,-2 ,-1 ,-1, -3, -3, -1, -1,  1,  3, -2, -1, -1, -2, -1, -2, -2, -1],\n",
    "       [2,  -2,  1,  0,  2,  0,  3, -2, -2, -1,  0, -1,  0, -2, -2, -2, -2, -2, -3, -2],\n",
    "       [0,  -1, -1, -2, -1, -1, -2, -3, -1, -1,  3,  4, -2,  0, -1, -2, -1, -1, -1, -1],\n",
    "       [0,  -3,  3,  1,  4,  1, -1, -3, -3, -1, -2,  0, -1, -3, -2, -3, -3, -3, -3, -3],\n",
    "       [0,  -1, -1, -2, -1, -1, -2, -3, -1, -1, -3,  4, -2,  0, -1, -2, -1, -1, -1, -1],\n",
    "       [-1, -2, -3, -3, -3, -3, -4, -4,  8, -3, -1, -1, -3, -2, -2, -2, -1, -2, -2, -1],\n",
    "       [3,   0, -2, -2, -1, -1, -2, -3, -1, -1,  3,  1, -2, -1, -1, -1, -1, -1, -1, -1],\n",
    "       [1,  -1 ,-2 ,-2 ,-1 ,-1, -3, -3, -1, -1,  4,  3, -2,  0,  0, -1, -1, -1, -1, -1],\n",
    "       [0,   6, -4, -4, -3 ,-3, -3, -3, -2, -3,  0, -2, -3, -1, -2, -2, -2, -3, -2, -2],\n",
    "       [0,  -3 , 1,  4,  1 , 2,  0, -2, -3, -1, -2, -1, -1, -3, -2, -3, -2, -2, -4, -3],\n",
    "       [2,   0 ,-2, -2, -1, -2, -3, -3, -1, -1,  4,  1, -2,  0,  0, -1,  0, -1, -1,  0],\n",
    "       [-2, -1 ,-3 ,-4 ,-3, -3, -4, -4, -2, -4,  0, -1, -3,  3,  0, -1, -1, -2,  6,  1],\n",
    "       [0 ,  6, -4, -4, -3, -3, -3, -3, -2, -3,  0, -2, -3, -1, -2, -2, -2, -3, -2, -2],\n",
    "       [1 , -1 ,-2 ,-2, -1, -1, -3, -2, -1, -2,  1,  3, -2,  0,  4, -1,  0,  0, -1,  0],\n",
    "       [0,  -2,  1, -1,  2,  0, -2, -3, -3, -1,  2,  3, -1, -1, -1, -2, -1, -2, -2, -1],\n",
    "       [0,  -3,  3,  1,  4,  1, -1, -3, -3, -1, -2,  0, -1, -3, -2, -3, -3, -3, -3, -3],\n",
    "       [0 , -1 ,-2, -2, -1, -1, -3, -3, -1, -2,  2,  3, -2,  0,  0, -1,  3,  1, -1,  0],\n",
    "       [0,  -3,  3,  1,  4,  1, -1, -3, -3, -1, -2,  0, -1, -3, -2, -3, -3, -3, -3, -3],\n",
    "       [3,   0, -2, -2, -1, -1, -3, -3, -1, -1,  3,  1, -2,  0,  0, -1, -1, -1, -1, -1]]\n",
    "    return M, positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_reshape(ds_idx=0):\n",
    "    global data_folder, dataset_file\n",
    "    ds = np.load(data_folder+dataset_file[ds_idx])\n",
    "    print(\"[INFO] The data set \"+dataset_file[ds_idx]+\"  has been loaded\")\n",
    "    ds = ds.reshape(-1,700,57)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels_from_samples(data):\n",
    "    \"\"\"\n",
    "    As indicated in the description above, the labels are then extracted from the\n",
    "    fetures loaded with the data and return with the data reshaped\n",
    "    note that the N- and C- terminals are being ignored as well as the relative and absolute solvent accessibility\n",
    "    and the sequence profile.\n",
    "    :param data: The data loaded\n",
    "    :return: data, labels\n",
    "    \"\"\"\n",
    "    labels_start = 22\n",
    "    labels_end = 31\n",
    "    aminoacids_end = 21\n",
    "\n",
    "    data_samples = data[:,:,:aminoacids_end]\n",
    "    labels  =data[:,:,labels_start:labels_end]\n",
    "    labels  = np.array([np.argmax(labels[i,:]) for i in range(labels.shape[0])])\n",
    "    return data_samples,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_scoring_matrix(data):\n",
    "    \"\"\"\n",
    "        Using scoring matrix defined in buildScoringMatrix in order to replace the one hot enconding\n",
    "        of the aminoacid with the encoding defined in the scoring matrix.\n",
    "        return: data set with the one hot encoded samples replaced by their corespondin position in the scoring matrix\n",
    "    \"\"\"\n",
    "    M, _ = buildScoringMatrix()\n",
    "    newData = []\n",
    "    for i,d in enumerate(data):\n",
    "        newData.append([])\n",
    "        for aminoacid in d:\n",
    "            aminoacid = np.dot(aminoacid,M).tolist()\n",
    "            #Replacing one hot enconding with aminoacid encoding from the scoring matrix\n",
    "            newData[i].append(aminoacid)\n",
    "        \n",
    "    print(\"[INFO] The dataset has had its one hot encoded aminoacids replaced by the ones in the scoring matrix\")\n",
    "    return np.array(newData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split(ds_opc):\n",
    "    \"\"\"\n",
    "    :param ds_opc: data set that has been selected\n",
    "           The ds_opc must indicate an option choice between the following sets\n",
    "           CB513.npy,cullpdb+profile_6133.npy.gz,cullpdb+profile_5926_filtered.npy.gz\n",
    "    :return:  train, test and validation sets (Already shuffled)\n",
    "    \"\"\"\n",
    "    assert type(ds_opc) == int\n",
    "    assert ds_opc < 3 and ds_opc >= 0\n",
    "    print(\"[INFO] The dataset \" + dataset_file[ds_opc] + \"Has been selected\")\n",
    "\n",
    "    data = load_and_reshape(ds_opc)\n",
    "    # Shuffling the data\n",
    "    data = sklearn.utils.shuffle(data)\n",
    "\n",
    "    data, labels = extract_labels_from_samples(data)\n",
    "    \n",
    "    data = use_scoring_matrix(data)\n",
    "    data = data.reshape(-1,data.shape[1],data.shape[2],1)\n",
    "\n",
    "    train_set = None\n",
    "    train_labels = None\n",
    "\n",
    "    test_set = None\n",
    "    test_labels = None\n",
    "\n",
    "    valid_set = None\n",
    "    valid_labels = None\n",
    "\n",
    "    if ds_opc == 0:\n",
    "        # spliting the cullpdb+profile_6133.npy.gz\n",
    "        train_idx = [0, 5430]\n",
    "        test_idx = [5435, 5690]\n",
    "        valid_idx = [5690, 5926]\n",
    "\n",
    "        train_set = data[:train_idx[1], :, :]\n",
    "        train_labels = labels[: train_idx[1]]\n",
    "\n",
    "        test_set = data[test_idx[0]:test_idx[1], :, :]\n",
    "        test_labels = labels[test_idx[0]:test_idx[1]]\n",
    "\n",
    "        valid_set = data[valid_idx[0]:valid_idx[1], :, :]\n",
    "        valid_labels = labels[valid_idx[0]:valid_idx[1]]\n",
    "\n",
    "    elif ds_opc == 1:\n",
    "        # spliting the cullpdb+profile_5926_filtered.npy.gz\n",
    "        train_idx = [0, 5600]\n",
    "        test_idx = [5605, 5877]\n",
    "        valid_idx = [5877, 6133]\n",
    "\n",
    "        train_set = data[:train_idx[1], :, :]\n",
    "        train_labels =labels[ :train_idx[1]]\n",
    "\n",
    "        test_set = data[test_idx[0]:test_idx[1], :, :]\n",
    "        test_labels = labels[test_idx[0]:test_idx[1]]\n",
    "\n",
    "        valid_set = data[valid_idx[0]:valid_idx[1], :, :]\n",
    "        valid_labels = labels[valid_idx[0]:valid_idx[1]]\n",
    "\n",
    "    elif ds_opc == 2:\n",
    "        # spliting CDB513\n",
    "        n_samples = 513\n",
    "        # Spliting the dataset in 70% for train, 20% for test and 10% for validation\n",
    "        train_val = int(n_samples * 0.7)\n",
    "        test_val = train_val + int(n_samples * 0.2)\n",
    "        valid_val = test_val + int(n_samples * 0.1)\n",
    "        train_idx = [0, train_val]\n",
    "        test_idx = [train_val, test_val]\n",
    "        valid_idx = [test_val, valid_val]\n",
    "\n",
    "        train_set = data[:train_idx[1], :, :]\n",
    "        train_labels = labels[: train_idx[1]]\n",
    "\n",
    "        test_set = data[test_idx[0]:test_idx[1], :, :]\n",
    "        test_labels = labels[test_idx[0]:test_idx[1]]\n",
    "\n",
    "        valid_set = data[valid_idx[0]:valid_idx[1], :, :]\n",
    "        valid_labels = labels[valid_idx[0]:valid_idx[1]]\n",
    "    print(\"[INFO] The dataset \" + dataset_file[ds_opc] + \"Has been split into train,test and validation sets\")\n",
    "    return train_set,train_labels, test_set,test_labels, valid_set,valid_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ConvNet(input_shape,n_classes):\n",
    "    \"\"\"\n",
    "    Simple comvolutional network that takes the whole protein to consideration\n",
    "    \"\"\"\n",
    "    model =  Sequential()\n",
    "    model.add(Conv2D(16,(5,5),input_shape=input_shape,use_bias=True,name=\"input_layer\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64,(5,5),use_bias=True,name=\"second_conv_layer\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128,(5,5),use_bias=True,name=\"third_conv_layer\"))\n",
    "    model.add(AveragePooling2D((2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64,activation='relu',name=\"first_fc_layer\"))\n",
    "    model.add(Dense(128,activation='relu',name=\"second_fc_layer\"))\n",
    "    model.add(Dense(64,activation='relu',name=\"third_fc_layer\"))\n",
    "    model.add(Dense(n_classes,name=\"classification\"))\n",
    "    print(\"[INFO] The model SimpleConvNet has been created\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_data,test_labels,batch_size,model,H):\n",
    "    ## Evaluating model\n",
    "    print(\"[INFO] Evaluating Network\")\n",
    "\n",
    "    predictions = model.predict(test_data, batch_size=batch_size)\n",
    "    value = classification_report(test_labels.argmax(axis=1),\n",
    "                                  predictions.argmax(axis=1))\n",
    "    print(value)\n",
    "\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, 20), H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, 20), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(np.arange(0, 20), H.history[\"acc\"], label=\"train_acc\")\n",
    "    plt.plot(np.arange(0, 20), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The dataset CB513.npyHas been selected\n",
      "[INFO] The data set CB513.npy  has been loaded\n",
      "[INFO] The dataset has had its one hot encoded aminoacids replaced by the ones in the scoring matrix\n",
      "[INFO] The dataset CB513.npyHas been split into train,test and validation sets\n",
      "[INFO] The model SimpleConvNet has been created\n",
      "[INFO] Training Network\n",
      "Train on 359 samples, validate on 102 samples\n",
      "Epoch 1/20\n",
      "180/359 [==============>...............] - ETA: 57s - loss: 11.6467 - acc: 0.0111 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-906a1ec6db7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m               validation_data=(test_set, test_labels))\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_set,train_labels,test_set,test_labels,valid_set,valid_labels = load_and_split(2)\n",
    "\n",
    "input_shape = train_set.shape[1:]\n",
    "\n",
    "n_classes = 9\n",
    "\n",
    "batch_size = 10\n",
    "lr = .001\n",
    "epochs = 20\n",
    "\n",
    "# Creating model\n",
    "model = simple_ConvNet(input_shape, n_classes)\n",
    "\n",
    "train_labels = keras.utils.to_categorical(train_labels, n_classes)\n",
    "test_labels = keras.utils.to_categorical(test_labels, n_classes)\n",
    "valid_labels = keras.utils.to_categorical(valid_labels, n_classes)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=lr, momentum=.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"[INFO] Training Network\")\n",
    "H = model.fit(train_set, train_labels,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(test_set, test_labels))\n",
    "\n",
    "evaluate_model(valid_set, valid_labels, batch_size, model, H)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
