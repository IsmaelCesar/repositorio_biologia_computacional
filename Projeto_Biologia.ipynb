{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4mOjp4IZhht"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, ActivityRegularization, BatchNormalization, Conv2D, AveragePooling2D, Flatten\n",
    "from keras.layers import Input,Add,MaxPooling2D,LSTM,TimeDistributed\n",
    "from keras.models  import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adamax\n",
    "import keras\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "janela = 25 #valor impar\n",
    "size = int(janela/2) #tamanho a ser incrementado nas laterais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ypmv5ArKZhiA"
   },
   "outputs": [],
   "source": [
    "def load(X):\n",
    "    data = X['prot']\n",
    "    classes = X['class']\n",
    "    positions = {'A':0,'C':1,'D':2,'E':3,'F':4,'G':5,'H':6,'I':7,'K':8,'L':9,'M':10,'N':11,\n",
    "                 'P':12,'Q':13,'R':14,'S':15,'T':16,'V':17,'W':18,'Y':19}\n",
    "    classes_converter = {'-': 0, 'E':1, 'H':2}\n",
    "    res = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            res.append(int(positions[data[i][j]])+1)\n",
    "    for i in range(size):\n",
    "        res.insert(0, 0)\n",
    "        res.append(0)\n",
    "    data =[]\n",
    "    for i in range(len(X['prot'])):\n",
    "        data.append(res[i:janela+i])\n",
    "        data[i].append(classes_converter[classes[i]])\n",
    "    columns = [[]]*(janela+1)\n",
    "    for i in range(janela):\n",
    "        columns[i] = \"Element\" + str(i)\n",
    "    columns[janela] = 'Class'\n",
    "    data = pd.DataFrame(data, columns = columns)\n",
    "    return data\n",
    "\n",
    "def load_data():\n",
    "    proteins = pd.read_csv('cb513.csv',sep='\\s*,\\s*')\n",
    "    res = load(proteins.iloc[0])\n",
    "    for i in range(1,len(proteins)):\n",
    "        aux = load(proteins.iloc[i])\n",
    "        res = res.append(aux,ignore_index = True)\n",
    "    print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Kg3flBKyL-O"
   },
   "outputs": [],
   "source": [
    "def buildScoringMatrix():\n",
    "    \"\"\"\n",
    "    Building protein scoring matrix and creating a dictionary with the aminoacids and each of their corresponding\n",
    "    rows in the matrix\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    positions = {'A':0,'C':1,'D':2,'E':3,'F':4,'G':5,'H':6,'I':7,'K':8,'L':9,'M':10,'N':11,\n",
    "                 'P':12,'Q':13,'R':14,'S':15,'T':16,'V':17,'W':18,'Y':19}\n",
    "    res = ['A','C','D','E','F','G','H','I','K','L','M','N',\n",
    "                 'P','Q','R','S','T','V','W','Y']\n",
    "    M=[[4,   0, -1, -2,  0, -1, -2, -3, -1, -1,  1,  0, -2, -2, -1, -2, -1, -2, -2, -1],\n",
    "       [-1, -2, -3 ,-3 ,-3 ,-3, -4, -4,  8, -3, -1, -1, -3, -2, -1, -2, -1, -2, -2, -1],\n",
    "       [3,   2, -2 ,-2 ,-1 ,-1, -3, -3, -1, -1,  1,  3, -2, -1, -1, -2, -1, -2, -2, -1],\n",
    "       [2,  -2,  1,  0,  2,  0,  3, -2, -2, -1,  0, -1,  0, -2, -2, -2, -2, -2, -3, -2],\n",
    "       [0,  -1, -1, -2, -1, -1, -2, -3, -1, -1,  3,  4, -2,  0, -1, -2, -1, -1, -1, -1],\n",
    "       [0,  -3,  3,  1,  4,  1, -1, -3, -3, -1, -2,  0, -1, -3, -2, -3, -3, -3, -3, -3],\n",
    "       [0,  -1, -1, -2, -1, -1, -2, -3, -1, -1, -3,  4, -2,  0, -1, -2, -1, -1, -1, -1],\n",
    "       [-1, -2, -3, -3, -3, -3, -4, -4,  8, -3, -1, -1, -3, -2, -2, -2, -1, -2, -2, -1],\n",
    "       [3,   0, -2, -2, -1, -1, -2, -3, -1, -1,  3,  1, -2, -1, -1, -1, -1, -1, -1, -1],\n",
    "       [1,  -1 ,-2 ,-2 ,-1 ,-1, -3, -3, -1, -1,  4,  3, -2,  0,  0, -1, -1, -1, -1, -1],\n",
    "       [0,   6, -4, -4, -3 ,-3, -3, -3, -2, -3,  0, -2, -3, -1, -2, -2, -2, -3, -2, -2],\n",
    "       [0,  -3 , 1,  4,  1 , 2,  0, -2, -3, -1, -2, -1, -1, -3, -2, -3, -2, -2, -4, -3],\n",
    "       [2,   0 ,-2, -2, -1, -2, -3, -3, -1, -1,  4,  1, -2,  0,  0, -1,  0, -1, -1,  0],\n",
    "       [-2, -1 ,-3 ,-4 ,-3, -3, -4, -4, -2, -4,  0, -1, -3,  3,  0, -1, -1, -2,  6,  1],\n",
    "       [0 ,  6, -4, -4, -3, -3, -3, -3, -2, -3,  0, -2, -3, -1, -2, -2, -2, -3, -2, -2],\n",
    "       [1 , -1 ,-2 ,-2, -1, -1, -3, -2, -1, -2,  1,  3, -2,  0,  4, -1,  0,  0, -1,  0],\n",
    "       [0,  -2,  1, -1,  2,  0, -2, -3, -3, -1,  2,  3, -1, -1, -1, -2, -1, -2, -2, -1],\n",
    "       [0,  -3,  3,  1,  4,  1, -1, -3, -3, -1, -2,  0, -1, -3, -2, -3, -3, -3, -3, -3],\n",
    "       [0 , -1 ,-2, -2, -1, -1, -3, -3, -1, -2,  2,  3, -2,  0,  0, -1,  3,  1, -1,  0],\n",
    "       [0,  -3,  3,  1,  4,  1, -1, -3, -3, -1, -2,  0, -1, -3, -2, -3, -3, -3, -3, -3],\n",
    "       [3,   0, -2, -2, -1, -1, -3, -3, -1, -1,  3,  1, -2,  0,  0, -1, -1, -1, -1, -1]]\n",
    "    M = np.array(M).transpose()\n",
    "    M = M.reshape(-1,21,1)\n",
    "    #M = pd.DataFrame(data=M,index=res)\n",
    "    return M\n",
    "\n",
    "def position_values_to_scores(data):\n",
    "    \"\"\"\n",
    "    :param data: The data must already be treated by the load_data and the load(X) procedures\n",
    "    the transformation is made only using the features\n",
    "    :return: data with each position value replaced by its corespondent column in the scoring matrix\n",
    "    \"\"\"\n",
    "    assert type(data) == np.ndarray\n",
    "    assert data.shape[1] == janela\n",
    "    zeros = np.zeros((21,1),dtype=np.int32).tolist()\n",
    "    M = buildScoringMatrix()\n",
    "    newData = []\n",
    "    for i,res in enumerate(data):\n",
    "        newData.append([])\n",
    "        for j,amin in enumerate(res):\n",
    "            if amin != 0:\n",
    "                newData[i].append(M[amin-1,:].tolist())\n",
    "            else:\n",
    "                newData[i].append(zeros)\n",
    "\n",
    "    return np.array(newData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwQeKNSlc_lc"
   },
   "source": [
    "### Model's Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Fwk-2qbdEXv"
   },
   "outputs": [],
   "source": [
    "def conv2DModel(input_shape,n_classes):\n",
    "    model = Sequential()\n",
    "    reg = 0.01\n",
    "    model.add(Conv2D(32,(3),input_shape=input_shape,use_bias=True,\n",
    "                   kernel_regularizer=regularizers.l2(reg),activation='relu'))\n",
    "    model.add(Conv2D(64,(3),use_bias=True,\n",
    "                   kernel_regularizer=regularizers.l2(reg),activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64,activation='relu',use_bias=True\n",
    "                  ,name=\"first_fc_layer\",kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(Dense(128,activation='tanh',use_bias=True\n",
    "                  ,name=\"second_fc_layer\",kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(Dense(128,activation='tanh',use_bias=True\n",
    "                  ,name=\"third_fc_layer\",kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(Dense(64,activation='relu',use_bias=True\n",
    "                  ,name=\"fourch_fc_layer\",kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(Dense(n_classes,activation='softmax',use_bias=True\n",
    "                        ,kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-HlZTTsTftp_"
   },
   "outputs": [],
   "source": [
    "def conv2DResnet(input_shape, n_classes):\n",
    "    reg = 0.01\n",
    "\n",
    "    # First Skip Layer 64\n",
    "    X_input = Input(input_shape)\n",
    "    X_skip = Conv2D(32, (3),use_bias=True,\n",
    "                    kernel_regularizer=regularizers.l2(reg), activation='relu')(X_input)\n",
    "    X = Conv2D(32, (3), use_bias=True,padding='same',\n",
    "               kernel_regularizer=regularizers.l2(reg),activation='relu')(X_skip)\n",
    "    X = Add()([X_skip, X])\n",
    "    X = BatchNormalization()(X)\n",
    "\n",
    "    # Third Skip Layer 512\n",
    "    X_skip = Conv2D(64, (3), use_bias=True,\n",
    "                    kernel_regularizer=regularizers.l2(reg), activation='relu')(X)\n",
    "    X = Conv2D(64, (3), padding='same',use_bias=True, \n",
    "               kernel_regularizer=regularizers.l2(reg),activation='relu')(X_skip)\n",
    "    X = Add()([X_skip, X])\n",
    "    X = BatchNormalization()(X)\n",
    "    X = MaxPooling2D(2)(X)\n",
    "\n",
    "    # Fully Connected part\n",
    "\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(64, activation='relu', name=\"first_fc_layer\", use_bias=True,\n",
    "              kernel_regularizer=regularizers.l2(reg))(X)\n",
    "    X = Dense(128, activation='relu', name=\"second_fc_layer\", use_bias=True,\n",
    "              kernel_regularizer=regularizers.l2(reg))(X)\n",
    "    X = Dense(64, activation='relu', name=\"third_fc_layer\", use_bias=True,\n",
    "              kernel_regularizer=regularizers.l2(reg))(X)\n",
    "    X = Dense(n_classes, activation='softmax', name=\"class\", use_bias=True\n",
    "              , kernel_regularizer=regularizers.l2(reg))(X)\n",
    "\n",
    "    model = Model(inputs=X_input, outputs=X, name=\"Conv1DResnet\")\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QmLD9hkhdGDQ"
   },
   "source": [
    "### Model's Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KGrutOryZhiQ",
    "outputId": "28b7edcc-5a44-4939-ec1f-815894b6c9ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismael/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Element0  Element1  Element2  Element3  Element4  Element5  Element6  \\\n",
      "0             0         0         0         0         0         0         0   \n",
      "1             0         0         0         0         0         0         0   \n",
      "2             0         0         0         0         0         0         0   \n",
      "3             0         0         0         0         0         0         0   \n",
      "4             0         0         0         0         0         0         0   \n",
      "5             0         0         0         0         0         0         0   \n",
      "6             0         0         0         0         0         0        16   \n",
      "7             0         0         0         0         0        16        14   \n",
      "8             0         0         0         0        16        14         8   \n",
      "9             0         0         0        16        14         8        15   \n",
      "10            0         0        16        14         8        15         7   \n",
      "11            0        16        14         8        15         7        20   \n",
      "12           16        14         8        15         7        20         9   \n",
      "13           14         8        15         7        20         9        19   \n",
      "14            8        15         7        20         9        19         4   \n",
      "15           15         7        20         9        19         4        18   \n",
      "16            7        20         9        19         4        18         4   \n",
      "17           20         9        19         4        18         4        20   \n",
      "18            9        19         4        18         4        20        11   \n",
      "19           19         4        18         4        20        11         5   \n",
      "20            4        18         4        20        11         5        19   \n",
      "21           18         4        20        11         5        19         1   \n",
      "22            4        20        11         5        19         1        13   \n",
      "23           20        11         5        19         1        13        12   \n",
      "24           11         5        19         1        13        12         2   \n",
      "25            5        19         1        13        12         2        12   \n",
      "26           19         1        13        12         2        12         4   \n",
      "27            1        13        12         2        12         4        12   \n",
      "28           13        12         2        12         4        12         8   \n",
      "29           12         2        12         4        12         8        18   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "11943         9        18        17        18        13        10         5   \n",
      "11944        18        17        18        13        10         5         4   \n",
      "11945        17        18        13        10         5         4         6   \n",
      "11946        18        13        10         5         4         6        18   \n",
      "11947        13        10         5         4         6        18        14   \n",
      "11948        10         5         4         6        18        14         9   \n",
      "11949         5         4         6        18        14         9        17   \n",
      "11950         4         6        18        14         9        17        14   \n",
      "11951         6        18        14         9        17        14        17   \n",
      "11952        18        14         9        17        14        17         8   \n",
      "11953        14         9        17        14        17         8        15   \n",
      "11954         9        17        14        17         8        15        16   \n",
      "11955        17        14        17         8        15        16         1   \n",
      "11956        14        17         8        15        16         1        16   \n",
      "11957        17         8        15        16         1        16         3   \n",
      "11958         8        15        16         1        16         3         8   \n",
      "11959        15        16         1        16         3         8        15   \n",
      "11960        16         1        16         3         8        15         3   \n",
      "11961         1        16         3         8        15         3        18   \n",
      "11962        16         3         8        15         3        18         5   \n",
      "11963         3         8        15         3        18         5         8   \n",
      "11964         8        15         3        18         5         8        12   \n",
      "11965        15         3        18         5         8        12         1   \n",
      "11966         3        18         5         8        12         1         6   \n",
      "11967        18         5         8        12         1         6         8   \n",
      "11968         5         8        12         1         6         8         9   \n",
      "11969         8        12         1         6         8         9         6   \n",
      "11970        12         1         6         8         9         6         4   \n",
      "11971         1         6         8         9         6         4         4   \n",
      "11972         6         8         9         6         4         4        20   \n",
      "\n",
      "       Element7  Element8  Element9  ...  Element16  Element17  Element18  \\\n",
      "0             0         0         0  ...          7         20          9   \n",
      "1             0         0         0  ...         20          9         19   \n",
      "2             0         0         0  ...          9         19          4   \n",
      "3             0         0        16  ...         19          4         18   \n",
      "4             0        16        14  ...          4         18          4   \n",
      "5            16        14         8  ...         18          4         20   \n",
      "6            14         8        15  ...          4         20         11   \n",
      "7             8        15         7  ...         20         11          5   \n",
      "8            15         7        20  ...         11          5         19   \n",
      "9             7        20         9  ...          5         19          1   \n",
      "10           20         9        19  ...         19          1         13   \n",
      "11            9        19         4  ...          1         13         12   \n",
      "12           19         4        18  ...         13         12          2   \n",
      "13            4        18         4  ...         12          2         12   \n",
      "14           18         4        20  ...          2         12          4   \n",
      "15            4        20        11  ...         12          4         12   \n",
      "16           20        11         5  ...          4         12          8   \n",
      "17           11         5        19  ...         12          8         18   \n",
      "18            5        19         1  ...          8         18         11   \n",
      "19           19         1        13  ...         18         11          6   \n",
      "20            1        13        12  ...         11          6          8   \n",
      "21           13        12         2  ...          6          8         12   \n",
      "22           12         2        12  ...          8         12          6   \n",
      "23            2        12         4  ...         12          6         14   \n",
      "24           12         4        12  ...          6         14          5   \n",
      "25            4        12         8  ...         14          5         13   \n",
      "26           12         8        18  ...          5         13          6   \n",
      "27            8        18        11  ...         13          6         13   \n",
      "28           18        11         6  ...          6         13         17   \n",
      "29           11         6         8  ...         13         17          8   \n",
      "...         ...       ...       ...  ...        ...        ...        ...   \n",
      "11943         4         6        18  ...         15         16          1   \n",
      "11944         6        18        14  ...         16          1         16   \n",
      "11945        18        14         9  ...          1         16          3   \n",
      "11946        14         9        17  ...         16          3          8   \n",
      "11947         9        17        14  ...          3          8         15   \n",
      "11948        17        14        17  ...          8         15          3   \n",
      "11949        14        17         8  ...         15          3         18   \n",
      "11950        17         8        15  ...          3         18          5   \n",
      "11951         8        15        16  ...         18          5          8   \n",
      "11952        15        16         1  ...          5          8         12   \n",
      "11953        16         1        16  ...          8         12          1   \n",
      "11954         1        16         3  ...         12          1          6   \n",
      "11955        16         3         8  ...          1          6          8   \n",
      "11956         3         8        15  ...          6          8          9   \n",
      "11957         8        15         3  ...          8          9          6   \n",
      "11958        15         3        18  ...          9          6          4   \n",
      "11959         3        18         5  ...          6          4          4   \n",
      "11960        18         5         8  ...          4          4         20   \n",
      "11961         5         8        12  ...          4         20          3   \n",
      "11962         8        12         1  ...         20          3          1   \n",
      "11963        12         1         6  ...          3          1          1   \n",
      "11964         1         6         8  ...          1          1         19   \n",
      "11965         6         8         9  ...          1         19         12   \n",
      "11966         8         9         6  ...         19         12         16   \n",
      "11967         9         6         4  ...         12         16          0   \n",
      "11968         6         4         4  ...         16          0          0   \n",
      "11969         4         4        20  ...          0          0          0   \n",
      "11970         4        20         3  ...          0          0          0   \n",
      "11971        20         3         1  ...          0          0          0   \n",
      "11972         3         1         1  ...          0          0          0   \n",
      "\n",
      "       Element19  Element20  Element21  Element22  Element23  Element24  Class  \n",
      "0             19          4         18          4         20         11      0  \n",
      "1              4         18          4         20         11          5      0  \n",
      "2             18          4         20         11          5         19      1  \n",
      "3              4         20         11          5         19          1      1  \n",
      "4             20         11          5         19          1         13      1  \n",
      "5             11          5         19          1         13         12      1  \n",
      "6              5         19          1         13         12          2      1  \n",
      "7             19          1         13         12          2         12      1  \n",
      "8              1         13         12          2         12          4      1  \n",
      "9             13         12          2         12          4         12      1  \n",
      "10            12          2         12          4         12          8      1  \n",
      "11             2         12          4         12          8         18      1  \n",
      "12            12          4         12          8         18         11      1  \n",
      "13             4         12          8         18         11          6      1  \n",
      "14            12          8         18         11          6          8      1  \n",
      "15             8         18         11          6          8         12      0  \n",
      "16            18         11          6          8         12          6      0  \n",
      "17            11          6          8         12          6         14      0  \n",
      "18             6          8         12          6         14          5      0  \n",
      "19             8         12          6         14          5         13      0  \n",
      "20            12          6         14          5         13          6      0  \n",
      "21             6         14          5         13          6         13      1  \n",
      "22            14          5         13          6         13         17      1  \n",
      "23             5         13          6         13         17          8      1  \n",
      "24            13          6         13         17          8         15      1  \n",
      "25             6         13         17          8         15          1      1  \n",
      "26            13         17          8         15          1         12      1  \n",
      "27            17          8         15          1         12          1      0  \n",
      "28             8         15          1         12          1          6      0  \n",
      "29            15          1         12          1          6          3      1  \n",
      "...          ...        ...        ...        ...        ...        ...    ...  \n",
      "11943         16          3          8         15          3         18      0  \n",
      "11944          3          8         15          3         18          5      0  \n",
      "11945          8         15          3         18          5          8      0  \n",
      "11946         15          3         18          5          8         12      0  \n",
      "11947          3         18          5          8         12          1      0  \n",
      "11948         18          5          8         12          1          6      0  \n",
      "11949          5          8         12          1          6          8      2  \n",
      "11950          8         12          1          6          8          9      2  \n",
      "11951         12          1          6          8          9          6      2  \n",
      "11952          1          6          8          9          6          4      2  \n",
      "11953          6          8          9          6          4          4      2  \n",
      "11954          8          9          6          4          4         20      2  \n",
      "11955          9          6          4          4         20          3      2  \n",
      "11956          6          4          4         20          3          1      2  \n",
      "11957          4          4         20          3          1          1      2  \n",
      "11958          4         20          3          1          1         19      2  \n",
      "11959         20          3          1          1         19         12      0  \n",
      "11960          3          1          1         19         12         16      0  \n",
      "11961          1          1         19         12         16          0      0  \n",
      "11962          1         19         12         16          0          0      0  \n",
      "11963         19         12         16          0          0          0      2  \n",
      "11964         12         16          0          0          0          0      2  \n",
      "11965         16          0          0          0          0          0      2  \n",
      "11966          0          0          0          0          0          0      2  \n",
      "11967          0          0          0          0          0          0      2  \n",
      "11968          0          0          0          0          0          0      2  \n",
      "11969          0          0          0          0          0          0      2  \n",
      "11970          0          0          0          0          0          0      2  \n",
      "11971          0          0          0          0          0          0      0  \n",
      "11972          0          0          0          0          0          0      0  \n",
      "\n",
      "[11973 rows x 26 columns]\n",
      "0.4143489518082352\n",
      "0.2382861438236031\n",
      "0.3473649043681617\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_data()\n",
    "\n",
    "coil = data.loc[data['Class'] == 0]\n",
    "fita = data.loc[data['Class'] == 1]\n",
    "helice = data.loc[data['Class'] == 2]\n",
    "\n",
    "print(len(coil)/len(data))\n",
    "print(len(fita)/len(data))\n",
    "print(len(helice)/len(data))\n",
    "sns.countplot(x='Class', data=data, palette='RdBu')\n",
    "\n",
    "X = data.drop(['Class'], axis=1)\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "a_NcpxJcZhil",
    "outputId": "102995dc-400a-48c0-b67a-328201b927d0"
   },
   "outputs": [],
   "source": [
    "X = np.asarray(X)\n",
    "\n",
    "X = position_values_to_scores(X)\n",
    "\n",
    "\n",
    "#standardizing data ====================================\n",
    "sample_height = X.shape[1]\n",
    "sample_width = X.shape[2]\n",
    "last_column = 1\n",
    "\n",
    "X  = X.reshape(-1,X.shape[1]*X.shape[2]*X.shape[3])\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "X = X.reshape(-1,sample_height,sample_width,last_column)\n",
    "#=======================================================\n",
    "\n",
    "\n",
    "X,y = sklearn.utils.shuffle([X,y])\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 3)\n",
    "y_test = keras.utils.to_categorical(y_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IplKmU83xq2d"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model,hist,X_test,y_test,batch_size,epochs):\n",
    "    predictions = model.predict(X_test, batch_size=batch_size)\n",
    "    value = classification_report(y_test.argmax(axis=1),\n",
    "                                  predictions.argmax(axis=1))\n",
    "    print(value)\n",
    "\n",
    "    # plot the training loss and accuracy\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    # plt.xscale('log')\n",
    "    # plt.yscale('log')\n",
    "    plt.ylim((0, 1))\n",
    "    plt.plot(np.arange(0, epochs), hist.history[\"acc\"], label=\"train_acc\")\n",
    "    plt.plot(np.arange(0, epochs), hist.history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.title(\"Training and validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LDIkYPchZhjk",
    "outputId": "d58e409e-3ffc-4c85-f5b0-6df229baeff1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size=512\n",
    "input_shape = X.shape[1:]\n",
    "print(\"Data shape\",input_shape)\n",
    "#model = conv2DModel(input_shape,3)\n",
    "model = conv2DResnet(input_shape,3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=.01,momentum=.5), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "evaluate_model(model, hist, X_test, y_test, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "AG4kJ4T11Cpz",
    "outputId": "4ce14d90-fe2b-4017-ae16-b8d93ecae224"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "p = [ np.argmax(p)  for p in predictions]\n",
    "df = pd.DataFrame(p,index=['class'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24QtamTpZhj3"
   },
   "outputs": [],
   "source": [
    "from keras import backend as k\n",
    "k.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42jqxQK7ZhkH"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = data.drop(['Class'], axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "print(classification_report(y_test, clf.predict(X_test)))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Projeto_Biologia.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
